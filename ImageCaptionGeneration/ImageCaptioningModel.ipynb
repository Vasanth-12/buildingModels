{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j07wdsxZBFhy"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2tCtFXw9iqS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as Keras\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, Conv2D, MaxPooling2D, LSTM, AdditiveAttention, Embedding, Dropout, TimeDistributed, AdditiveAttention, Reshape\n",
        "from keras.optimizers import Adam, SGD\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tempfile\n",
        "\n",
        "import opendatasets as od\n",
        "import os\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR5X53pCjsuG"
      },
      "outputs": [],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPvrnmM9KJfd"
      },
      "source": [
        "Problems I faced during this Project:\n",
        "\n",
        "\n",
        "1. Converting image to numpy array requires RAM.\n",
        "> Solution tried,\n",
        "        1. OpenCV packages to process images\n",
        "        2. Image PIL package\n",
        "        3. pySpark for InMemory space\n",
        "\n",
        "  Failed at every try and finally found numpy.memmap\n",
        "  This memmap store the data in disk and load the data from disk on request\n",
        "\n",
        "2. Creating custom CNN for this project instead of pre-trained model\n",
        "> building model based on the desired input size will eat your RAM. I build the CNN input layer with 500*500*3 with ResNet-50 architecture. That day I found building the model requires the RAM since we need to store the neural network weight.\n",
        "> Solution worked\n",
        "        1. reduced the input layer dimension\n",
        "        2. removed some convolutional and pooling layer\n",
        "  Once reducing the input layer dimension, the model size reduced from GBs to MBs\n",
        "\n",
        "3. model.fit loads the data into RAM and then train the model. But our input data doesn't fit in RAM. So, approached different solution\n",
        "> Solutions,\n",
        "          1. fit_generator - deprecated in Keras\n",
        "          2. train_on_batch\n",
        "\n",
        "4. Attention layer's Context vector doesn't fit with Embedding layer. Context Vector will be produced at each timestep. But Embedding layer (during training), embeds all the input at a time. Eg, in my model it takes 20 words. Embedding layer embeds 20 words at a time. But context vector should be generated at timestep\n",
        "> Solution\n",
        "          1. Placed the Attention layer after LSTM. Most of the time Attenstion layers are used after LSTM in NLP.\n",
        "\n",
        "5. sklearn test_train_split packes  requires RAM\n",
        "> Solution,\n",
        "          1. using panda DataFrame using frac\n",
        "          2. separate last 20% of data for testing\n",
        "\n",
        "6. Creating predtion model from trained model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlOUFOAGzKoC"
      },
      "outputs": [],
      "source": [
        "!rm -r ./flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0vRYNMPBXl-"
      },
      "outputs": [],
      "source": [
        "od.download('https://www.kaggle.com/datasets/adityajn105/flickr8k?resource=download&select=Images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C1F2MvUBlz_"
      },
      "outputs": [],
      "source": [
        "captionData = pd.read_csv('./flickr8k/captions.txt', delimiter='.jpg,', header=0, names=['image', 'caption'])\n",
        "captionData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpzjVmqgTIx9"
      },
      "outputs": [],
      "source": [
        "#captionData = captionData.sample(frac=1)\n",
        "#captionData.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEqoc6NngCtE"
      },
      "outputs": [],
      "source": [
        "# No. of captions\n",
        "captionData.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbolWfeUgJtU"
      },
      "outputs": [],
      "source": [
        "# No. of images\n",
        "!ls ./flickr8k/Images | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kV89XsB0Vb7"
      },
      "outputs": [],
      "source": [
        "newDimensionL, newDimensionB, newChannel = (250, 250, 3)\n",
        "padColorRGB = (0, 0, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98gpqGk0_vuR"
      },
      "outputs": [],
      "source": [
        "totalDataSize = captionData['caption'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5cbDj8Q742o"
      },
      "outputs": [],
      "source": [
        "# Working piece of code\n",
        "'''\n",
        "imgArrayDictionary = {}\n",
        "count = 0\n",
        "imageList = []\n",
        "\n",
        "for imgFile in os.listdir('./flickr8k/Images'):\n",
        "  fullPath = './flickr8k/Images/' + imgFile\n",
        "  img = Keras.utils.load_img(fullPath)\n",
        "\n",
        "  img = img.resize((newDimensionL, newDimensionB))\n",
        "  imgArray = Keras.utils.img_to_array(img, dtype=np.float16) / 255\n",
        "\n",
        "  imgArrayDictionary[imgFile] = imgArray\n",
        "\n",
        "  count += 1\n",
        "  if (count % 100 == 0):\n",
        "    print(count)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K4hT_4tug14"
      },
      "outputs": [],
      "source": [
        "horizontalImgGenerator = ImageDataGenerator(\n",
        "  horizontal_flip=True\n",
        ")\n",
        "\n",
        "tiltedImgGenerator = ImageDataGenerator(\n",
        "  rotation_range=10\n",
        ")\n",
        "\n",
        "# Combined one\n",
        "imgGenerator = ImageDataGenerator(\n",
        "  horizontal_flip=True,\n",
        "  rotation_range=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E8JY7huoixPj"
      },
      "outputs": [],
      "source": [
        "# creating tempfile to store the numpy array in disk\n",
        "tempFileName = tempfile.TemporaryFile()\n",
        "imageMemFile = np.memmap(tempFileName, dtype=np.float16, shape=(totalDataSize, newDimensionL, newDimensionB, newChannel), mode='w+')\n",
        "\n",
        "count = 0\n",
        "\n",
        "for imageName in captionData['image']:\n",
        "\n",
        "  fullPath = './flickr8k/Images/' + imageName + '.jpg'\n",
        "  img = Keras.utils.load_img(fullPath)\n",
        "\n",
        "  img = img.resize((newDimensionL, newDimensionB))\n",
        "  imgArray = Keras.utils.img_to_array(img, dtype=np.float16) / 255\n",
        "\n",
        "  dummyArray = np.zeros((1, newDimensionL, newDimensionB, newChannel))\n",
        "  dummyArray[0] = img\n",
        "  if count%5 == 1:\n",
        "    # For Horizontal Flip Image Generation\n",
        "    augumentIterator = horizontalImgGenerator.flow(dummyArray, batch_size=1)\n",
        "    augumentedImageArray = next(augumentIterator)[0].astype('uint8')\n",
        "\n",
        "    imgArray = Keras.utils.img_to_array(augumentedImageArray, dtype=np.float16) / 255\n",
        "\n",
        "  elif count%5 == 2:\n",
        "    # For Tilted Image Generation\n",
        "    augumentIterator = tiltedImgGenerator.flow(dummyArray, batch_size=1)\n",
        "    augumentedImageArray = next(augumentIterator)[0].astype('uint8')\n",
        "\n",
        "    imgArray = Keras.utils.img_to_array(augumentedImageArray, dtype=np.float16) / 255\n",
        "\n",
        "  elif count%5 == 3 or count%5 == 4:\n",
        "    # Combined Image Generator\n",
        "    augumentIterator = imgGenerator.flow(dummyArray, batch_size=1)\n",
        "    augumentedImageArray = next(augumentIterator)[0].astype('uint8')\n",
        "\n",
        "    imgArray = Keras.utils.img_to_array(augumentedImageArray, dtype=np.float16) / 255\n",
        "\n",
        "  imageMemFile[count] = imgArray\n",
        "  count += 1\n",
        "\n",
        "  # Working piece\n",
        "  '''\n",
        "  #imgArray = imageToArray[imgArrayDictionary[imageName]]\n",
        "  imgArray = imgArrayDictionary[imageName]\n",
        "  imageMemFile[count] = imgArray\n",
        "  count += 1\n",
        "  '''\n",
        "\n",
        "  if (count % 500 == 0):\n",
        "    print(count)\n",
        "\n",
        "print('Image count: ', imageMemFile.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a01uDES_UsGK"
      },
      "outputs": [],
      "source": [
        "captions = []\n",
        "for captionText in captionData['caption']:\n",
        "  captionText = re.sub('[^\\w^\\s^\\']', '', captionText.lower())\n",
        "  captionText = re.sub('\\s+', ' ', captionText)\n",
        "  captions.append(captionText)\n",
        "\n",
        "print('Caption count: ', len(captions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLFd3w_SyVl7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZM1XoZNeSSn"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm', exclude=['ner', 'parser', 'attribute_ruler', 'tagger', 'lemmatizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4HLDoqXof5j"
      },
      "outputs": [],
      "source": [
        "# Hyper parameter\n",
        "############################\n",
        "threshold = 3\n",
        "maxPadLength = 20\n",
        "############################\n",
        "\n",
        "vocab = {}\n",
        "sentLength = []\n",
        "index = 4\n",
        "\n",
        "# This loop is to get vocabulary count\n",
        "for caption in captions:\n",
        "  sentLength.append(len(caption.split(' ')))\n",
        "  #for word in caption.split(' '):\n",
        "  doc = nlp(caption)\n",
        "  for token in doc:\n",
        "    word = token.text\n",
        "\n",
        "    if vocab.get(word):\n",
        "      vocab[word]['count'] += 1\n",
        "    else:\n",
        "      #vocab[word]['count'] = 1\n",
        "      #vocab[word]['index'] = index\n",
        "      vocab[word] = {'count': 1, 'index': index, 'embedding': token.vector}\n",
        "      index += 1\n",
        "\n",
        "# indexing special token\n",
        "#   here count = 10, is just a dummy value\n",
        "vocab['<PAD>'] = {'count': 10, 'index': 0, 'embedding': np.random.uniform(-1, 1, (96,))}\n",
        "vocab['<SOS>'] = {'count': 10, 'index': 1, 'embedding': np.random.uniform(-1, 1, (96,))}\n",
        "vocab['<EOS>'] = {'count': 10, 'index': 2, 'embedding': np.random.uniform(-1, 1, (96,))}\n",
        "vocab['<OOV>'] = {'count': 10, 'index': 3, 'embedding': np.random.uniform(-1, 1, (96,))}\n",
        "\n",
        "plt.hist(sorted(sentLength))\n",
        "plt.show()\n",
        "\n",
        "# index to word mapping\n",
        "idx2word = {}\n",
        "for key in vocab.keys():\n",
        "  idx = vocab[key]['index']\n",
        "  idx2word[idx] = key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl7sK5pXq1Q7"
      },
      "outputs": [],
      "source": [
        "with open('index_to_word.json', 'w') as f:\n",
        "    json.dump(idx2word, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-h1F2p01o7I"
      },
      "outputs": [],
      "source": [
        "tempCaptionFile = tempfile.TemporaryFile()\n",
        "#tempCaptionFile.detach()\n",
        "captionMemFile = np.memmap(tempCaptionFile, mode='w+', dtype=np.uint32, shape=(totalDataSize,maxPadLength))\n",
        "captionMemFile.flush()\n",
        "\n",
        "print(captionMemFile.shape)\n",
        "\n",
        "count = 0\n",
        "for caption in captions:\n",
        "  captionIndex = []\n",
        "  captionIndex.append(vocab['<SOS>']['index'])\n",
        "  #for word in caption.split(' '):\n",
        "  doc = nlp(caption)\n",
        "  for token in doc:\n",
        "    word = token.text\n",
        "    if word == '':\n",
        "      continue\n",
        "    if vocab[word]['count'] < threshold:\n",
        "      captionIndex.append(vocab['<OOV>']['index'])\n",
        "    else:\n",
        "      captionIndex.append(vocab[word]['index'])\n",
        "    #captionIndex.append([vocab[word]['index']])\n",
        "  captionIndex.append(vocab['<EOS>']['index'])\n",
        "\n",
        "  captionArray = np.asarray(captionIndex)\n",
        "  padLength = maxPadLength - len(captionIndex)\n",
        "  if padLength < 0:\n",
        "    captionMemFile[count] = captionArray[0: maxPadLength]\n",
        "    captionMemFile[count][maxPadLength-1] = 2\n",
        "  else:\n",
        "    captionMemFile[count] = np.pad(captionArray, (0, padLength))\n",
        "  count += 1\n",
        "\n",
        "  if (count % 500 == 0):\n",
        "    print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRfOTM3dy7Xv"
      },
      "outputs": [],
      "source": [
        "vocab['family']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6zywxcD3QNN"
      },
      "outputs": [],
      "source": [
        "vocab['family']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8-50Bmp35Kw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plq_w2pWqCEW"
      },
      "outputs": [],
      "source": [
        "# create embedding weight for embedding layer\n",
        "embeddingMatrix = np.zeros((index+1, 96))\n",
        "for word in vocab.keys():\n",
        "  i = vocab[word]['index']\n",
        "  embeddingMatrix[i] = vocab[word]['embedding']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbaViXgNZ3Lp"
      },
      "outputs": [],
      "source": [
        "embeddingMatrix[3000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qQffXle8P_a"
      },
      "outputs": [],
      "source": [
        "# To verify everything as expected\n",
        "print(captions[21003])\n",
        "print(captionMemFile[21003])\n",
        "sampleImage = imageMemFile[21003] * 255\n",
        "\n",
        "plt.imshow(sampleImage.astype(int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VUwROwK6zeb"
      },
      "outputs": [],
      "source": [
        "# To verify everything as expected\n",
        "print(captions[21003])\n",
        "print(captionMemFile[21003])\n",
        "sampleImage = imageMemFile[21003] * 255\n",
        "\n",
        "plt.imshow(sampleImage.astype(int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSLDM-Wj62zR"
      },
      "outputs": [],
      "source": [
        "# To verify everything as expected\n",
        "print(captions[3000])\n",
        "print(captionMemFile[3000])\n",
        "sampleImage = imageMemFile[3000] * 255\n",
        "\n",
        "plt.imshow(sampleImage.astype(int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLc2QyA47kr7"
      },
      "outputs": [],
      "source": [
        "# To verify everything as expected\n",
        "print(captions[3000])\n",
        "print(captionMemFile[3000])\n",
        "sampleImage = imageMemFile[3000] * 255\n",
        "\n",
        "plt.imshow(sampleImage.astype(int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuAtU--V7nJ_"
      },
      "outputs": [],
      "source": [
        "# To verify everything as expected\n",
        "print(captions[39374])\n",
        "print(captionMemFile[39374])\n",
        "sampleImage = imageMemFile[39374] * 255\n",
        "\n",
        "plt.imshow(sampleImage.astype(int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOYVWLII8g8d"
      },
      "outputs": [],
      "source": [
        "# To verify everything as expected\n",
        "print(captions[39374])\n",
        "print(captionMemFile[39374])\n",
        "sampleImage = imageMemFile[39374] * 255\n",
        "\n",
        "plt.imshow(sampleImage.astype(int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qlg5G_nDnlRe"
      },
      "outputs": [],
      "source": [
        "# Split data for training and testing\n",
        "# working pieceof code\n",
        "trainDataSizePercetage = 80\n",
        "\n",
        "limit = totalDataSize * trainDataSizePercetage // 100\n",
        "# Train Data\n",
        "xTrain, yTrain = imageMemFile[:limit], captionMemFile[:limit]\n",
        "\n",
        "# Test Data\n",
        "xTest, yTest = imageMemFile[limit:], captionMemFile[limit:]\n",
        "\n",
        "print(xTrain.shape[0], \" -> \", yTrain.shape[0])\n",
        "print(xTest.shape[0], \" -> \", yTest.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXwhAzGc-lXv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5fnC2J19Vva"
      },
      "outputs": [],
      "source": [
        "tempTrainFileName = tempfile.TemporaryFile()\n",
        "xTrain = np.memmap(tempTrainFileName, dtype=np.float16, shape=(32364, newDimensionL, newDimensionB, newChannel), mode='w+')\n",
        "tempTestFileName = tempfile.TemporaryFile()\n",
        "xTest = np.memmap(tempTestFileName, dtype=np.float16, shape=(8091, newDimensionL, newDimensionB, newChannel), mode='w+')\n",
        "\n",
        "tempTrainCaptionFile = tempfile.TemporaryFile()\n",
        "yTrain = np.memmap(tempTrainCaptionFile, mode='w+', dtype=np.uint32, shape=(32364,maxPadLength))\n",
        "tempTestCaptionFile = tempfile.TemporaryFile()\n",
        "yTest = np.memmap(tempTestCaptionFile, mode='w+', dtype=np.uint32, shape=(8091,maxPadLength))\n",
        "\n",
        "xTest, yTest = imageMemFile[4::5], captionMemFile[4::5]\n",
        "print(xTest.shape[0], \" -> \", yTest.shape[0])\n",
        "\n",
        "exceptList = []\n",
        "for i in range(1,8092):\n",
        "  exceptList.append(i*5)\n",
        "\n",
        "len(exceptList)\n",
        "\n",
        "xTrain = np.delete(imageMemFile, exceptList)\n",
        "yTrain = np.delete(captionMemFile, exceptList)\n",
        "print(xTrain.shape[0], \" -> \", yTrain.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjatGE5qZLxo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1,2,3,4,5,6,7,8,9,0])\n",
        "xyz = [2,5,8]\n",
        "\n",
        "b = a[4::5]\n",
        "c = np.delete(a, xyz)\n",
        "\n",
        "print(b)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPIRICpbB7_-"
      },
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQNOCRxFaefP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "array = np.array([1, 2, 3, 4, 5])\n",
        "indices = [i for i in range(len(array)) if i % 2 == 0]\n",
        "subarrays = np.split(array, indices)\n",
        "print(subarrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycHlzI8jcC-N"
      },
      "outputs": [],
      "source": [
        "arr = np.array([5,7,9,11,13,19,23,27])\n",
        "subarrays = np.split(arr, [2, 5, 7])\n",
        "\n",
        "print(subarrays)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4uFJRCtVkS1"
      },
      "outputs": [],
      "source": [
        "limit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8H8_m_w-mt-"
      },
      "outputs": [],
      "source": [
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60M6vQQpmkqF"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "############################\n",
        "batchSize = 15\n",
        "epochs = 15\n",
        "############################\n",
        "vocabSize = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBz3ooUWuWoz"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def encoderModel(vocabSize):\n",
        "\n",
        "  inputLayer = Input(shape=(500,500,3), batch_size=batchSize)\n",
        "  conv1 = Conv2D(filters=64, kernel_size=(2,2), padding='same', activation='relu')(inputLayer)\n",
        "  #conv2 = Conv2D(filters=64, kernel_size=(2,2), padding='same', activation='relu')(conv1)\n",
        "\n",
        "  maxPool1 = MaxPooling2D(pool_size=(20,20), strides=(4,4), padding='valid')(conv1)\n",
        "  conv3 = Conv2D(filters=128, kernel_size=(2,2), padding='same', activation='relu')(maxPool1)\n",
        "\n",
        "  maxPool2 = MaxPooling2D(pool_size=(4,4), strides=(2,2), padding='valid')(conv3)\n",
        "  conv5 = Conv2D(filters=256, kernel_size=(2,2), padding='same', activation='relu')(maxPool2)\n",
        "\n",
        "  maxPool3 = MaxPooling2D(pool_size=(4,4), strides=(2,2), padding='valid')(conv5)\n",
        "  conv8 = Conv2D(filters=512, kernel_size=(2,2), padding='same', activation='relu')(maxPool3)\n",
        "\n",
        "  maxPool4 = MaxPooling2D(pool_size=(8,8), strides=(2,2), padding='valid')(conv8)\n",
        "  flattenNetwork = Flatten()(maxPool4)\n",
        "\n",
        "  dropout1 = Dropout(0.3)(flattenNetwork)\n",
        "  dense1 = Dense(2048, activation='relu')(dropout1)\n",
        "  dropout2 = Dropout(0.3)(dense1)\n",
        "  dense2 = Dense(1024, activation='relu')(dense1)\n",
        "\n",
        "  #outputLayer = Dense(1024, activation='softmax')\n",
        "\n",
        "  model = Keras.Model(\n",
        "      inputs = [inputLayer],\n",
        "      outputs = [dense2, dense2]\n",
        "  )\n",
        "\n",
        "  return model, [model.layers[-1].output, model.layers[-1].output]\n",
        "\n",
        "encoder, encoderHiddenStates = encoderModel(vocabSize)\n",
        "encoderOptimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "print(encoder.summary())\n",
        "Keras.utils.plot_model(encoder, show_shapes=True)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNC3WT4hlZ62"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def decoderModel(encoderHiddenStates, vocabSize):\n",
        "\n",
        "  deinputLayer = Input(shape=(1,), batch_size=batchSize)\n",
        "  embeddingLayer = Embedding(vocabSize+1, 100, input_length=10)(deinputLayer)\n",
        "  decoderOutput, decoderHidden, decoderCell = LSTM(1024, activation='relu', return_sequences=True, return_state=True)(embeddingLayer, initial_state=encoderHiddenStates)\n",
        "  decoderDense = Dense(vocabSize, activation='sigmoid')\n",
        "  decoderTimeDist = TimeDistributed(decoderDense)(decoderOutput)\n",
        "\n",
        "  model = Keras.Model(\n",
        "      inputs = [eninputLayer, encoderHiddenStates],\n",
        "      outputs = decoderTimeDist\n",
        "  )\n",
        "\n",
        "  return model, [decoderHidden, decoderCell]\n",
        "\n",
        "decoder, decoderStates = decoderModel(encoderHiddenStates, vocabSize)\n",
        "decoderOptimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "print(decoder.summary())\n",
        "Keras.utils.plot_model(decoder, show_shapes=True)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFghpvYY8XfO"
      },
      "outputs": [],
      "source": [
        "def imageCaptionModel():\n",
        "  inputLayer = Input(shape=(newDimensionL, newDimensionB, 3), batch_size=None)\n",
        "  conv1 = Conv2D(filters=64, kernel_size=(2,2), padding='same', activation='relu')(inputLayer)\n",
        "  conv2 = Conv2D(filters=64, kernel_size=(2,2), padding='same', activation='sigmoid')(conv1)\n",
        "\n",
        "  maxPool1 = MaxPooling2D(pool_size=(20,20), strides=(4,4), padding='valid')(conv2)\n",
        "  conv3 = Conv2D(filters=128, kernel_size=(2,2), padding='same', activation='relu')(maxPool1)\n",
        "  conv4 = Conv2D(filters=128, kernel_size=(2,2), padding='same', activation='sigmoid')(conv3)\n",
        "\n",
        "  maxPool2 = MaxPooling2D(pool_size=(4,4), strides=(2,2), padding='valid')(conv4)\n",
        "  conv5 = Conv2D(filters=256, kernel_size=(2,2), padding='same', activation='relu')(maxPool2)\n",
        "  conv6 = Conv2D(filters=256, kernel_size=(2,2), padding='same', activation='sigmoid')(conv5)\n",
        "\n",
        "  maxPool3 = MaxPooling2D(pool_size=(4,4), strides=(2,2), padding='valid')(conv6)\n",
        "  conv7 = Conv2D(filters=512, kernel_size=(2,2), padding='same', activation='relu')(maxPool3)\n",
        "  conv8 = Conv2D(filters=1024, kernel_size=(2,2), padding='same', activation='relu')(conv7)\n",
        "\n",
        "  cNNHiddenState = Reshape((169, 1024))(conv8)\n",
        "\n",
        "  maxPool4 = MaxPooling2D(pool_size=(8,8), strides=(2,2), padding='valid')(conv8)\n",
        "  flattenNetwork = Flatten()(maxPool4)\n",
        "\n",
        "  dropout1 = Dropout(0.1)(flattenNetwork)\n",
        "  dense1 = Dense(2048, activation='relu')(dropout1)\n",
        "  dropout2 = Dropout(0.1)(dense1)\n",
        "  dense2 = Dense(1024, activation='sigmoid')(dropout2)\n",
        "\n",
        "  encoderOutputAsState = [dense2, dense2]\n",
        "\n",
        "  deinputLayer = Input(shape=(1), batch_size=None)\n",
        "  embeddingLayer = Embedding(vocabSize+1, 96, input_length=1, weights=[embeddingMatrix], trainable=False)(deinputLayer)\n",
        "\n",
        "  lstmOutput, decoderHidden, decoderCell = LSTM(1024, activation='relu', return_sequences=True, return_state=True)(embeddingLayer, initial_state=encoderOutputAsState)\n",
        "  contextVector, attentionScore = AdditiveAttention()([lstmOutput, cNNHiddenState, cNNHiddenState], return_attention_scores=True)\n",
        "\n",
        "  decoderDense = Dense(index, activation='softmax')(contextVector)\n",
        "\n",
        "  imgCaptionModel = Keras.Model(\n",
        "      inputs = [inputLayer, deinputLayer],\n",
        "      outputs = decoderDense\n",
        "  )\n",
        "  return imgCaptionModel, inputLayer, dense2, cNNHiddenState, deinputLayer, decoderDense, decoderHidden, decoderCell\n",
        "\n",
        "imgCaptionModel, encoderInputLayer, encoderOutputLayer, cNNHiddenState, decoderInputLayer, decoderOutputLayer, decoderHidden, decoderCell  = imageCaptionModel()\n",
        "\n",
        "print(imgCaptionModel.summary())\n",
        "Keras.utils.plot_model(imgCaptionModel, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjsxUAY-E5Yc"
      },
      "outputs": [],
      "source": [
        "imgCaptionModel.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=[Keras.metrics.SparseCategoricalCrossentropy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLPi3NQMOVWB"
      },
      "outputs": [],
      "source": [
        "# This is purely for training purpose\n",
        "model = Keras.models.load_model('imageCaptioningModel_Epoch13.keras')\n",
        "\n",
        "# remove this block\n",
        "# due to GPU sortage, I'm doing this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a93r_dGjmLIc"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8THTIQoSYgpT"
      },
      "outputs": [],
      "source": [
        "batchForTraining = (xTrain.shape[0] // batchSize)\n",
        "batchForTesting = (xTest.shape[0] // batchSize)\n",
        "\n",
        "trainingLossHistory = []\n",
        "trainingAccuracyHistory = []\n",
        "testingLossHistory = []\n",
        "testingAccuracyHistory = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epochTrainingLoss, epochTrainingAccuracy, epochTestingLoss, epochTestingAccuracy = 0, 0, 0, 0\n",
        "  for batch in tqdm(range(batchForTraining)):\n",
        "    batchIndex = batch * batchSize\n",
        "    batchLimit = batchIndex + batchSize\n",
        "    loss, accuracy = imgCaptionModel.train_on_batch(x=[xTrain[batchIndex:batchLimit],yTrain[batchIndex:batchLimit]], y=yTrain[batchIndex:batchLimit])\n",
        "    epochTrainingLoss += loss\n",
        "    epochTrainingAccuracy += accuracy\n",
        "\n",
        "  # Average loss and accuracy of Training Dataset per epoch\n",
        "  epochTrainingLoss /= batchForTraining\n",
        "  epochTrainingAccuracy /= batchForTraining\n",
        "\n",
        "  trainingLossHistory.append(epochTrainingLoss)\n",
        "  trainingAccuracyHistory.append(epochTrainingAccuracy)\n",
        "\n",
        "  for batch in tqdm(range(batchForTesting)):\n",
        "    batchIndex = batch * batchSize\n",
        "    batchLimit = batchIndex + batchSize\n",
        "    loss, accuracy = imgCaptionModel.test_on_batch(x=[xTrain[batchIndex:batchLimit],yTrain[batchIndex:batchLimit]], y=yTrain[batchIndex:batchLimit])\n",
        "    epochTestingLoss += loss\n",
        "    epochTestingAccuracy += accuracy\n",
        "\n",
        "  # Average loss and accuracy of Testing Dataset per epoch\n",
        "  epochTestingLoss /= batchForTesting\n",
        "  epochTestingAccuracy /= batchForTesting\n",
        "\n",
        "  testingLossHistory.append(epochTestingLoss)\n",
        "  testingAccuracyHistory.append(epochTestingAccuracy)\n",
        "\n",
        "  print(\"EPOCH: \", epoch, \" Training Loss: \", epochTrainingLoss, \" Training Accuracy: \", epochTrainingAccuracy, \"\\t\\tTesting Loss: \", epochTestingLoss ,\" Testing Accuracy: \", epochTestingAccuracy)\n",
        "\n",
        "  #modelName = \"imageCaptioningModel_Epoch\" + str(epoch) + \".keras\"\n",
        "  #imgCaptionModel.save(modelName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r-cfcciJMS5"
      },
      "outputs": [],
      "source": [
        "imgCaptionModel.save('imageCaptionModel.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhCDJgg8RLV6"
      },
      "outputs": [],
      "source": [
        "imgTest = Keras.utils.load_img('./flickr8k/Images/101669240_b2d3e7f17b.jpg')\n",
        "\n",
        "imgTest = imgTest.resize((newDimensionL, newDimensionB))\n",
        "imgTestArray = Keras.utils.img_to_array(imgTest) / 255\n",
        "\n",
        "plt.imshow(imgTestArray)\n",
        "plt.show()\n",
        "\n",
        "imgTestArray = Keras.utils.img_to_array(imgTest, dtype=np.float16) / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2iADqReko3g"
      },
      "outputs": [],
      "source": [
        "print(imgTestArray.shape)\n",
        "imgTestArray = imgTestArray.reshape(1, 250, 250, 3)\n",
        "print(imgTestArray.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOoRM00ODI-D"
      },
      "outputs": [],
      "source": [
        "trainedImgCaptionModel = Keras.models.load_model('imageCaptioningModel_Epoch13.keras')\n",
        "#Keras.utils.plot_model(trainedImgCaptionModel, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oNWiZWQmARC"
      },
      "outputs": [],
      "source": [
        "encoderPredictModel = Keras.Model(\n",
        "  trainedImgCaptionModel.get_layer('input_1').input,\n",
        "  [trainedImgCaptionModel.get_layer('dense_1').output] + [trainedImgCaptionModel.get_layer('conv2d_7').output]\n",
        ")\n",
        "encoderOutputTensorAsState = [encoderPredictModel.get_layer('dense_1').output, encoderPredictModel.get_layer('dense_1').output]\n",
        "\n",
        "encoderPredictModel.save(\"imageCaptionEncoderModel.keras\")\n",
        "#print(encoderPredictModel.summary())\n",
        "#Keras.utils.plot_model(encoderPredictModel, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG_KPdcyUREa"
      },
      "outputs": [],
      "source": [
        "# Building the decoder model for prediction\n",
        "decoderPredInput = trainedImgCaptionModel.get_layer('input_2').input\n",
        "decoderPredEmbedding = trainedImgCaptionModel.get_layer('embedding')(decoderPredInput)\n",
        "\n",
        "decoderPredLSTM = trainedImgCaptionModel.get_layer('lstm')\n",
        "decoderPredLSTMOutput, decoderPredHidden, decoderPredCell = decoderPredLSTM(decoderPredEmbedding, initial_state=encoderOutputTensorAsState)\n",
        "\n",
        "decoderPredStates = [decoderPredHidden, decoderPredCell]\n",
        "\n",
        "encoderHiddenState = encoderPredictModel.get_layer('conv2d_7').output\n",
        "reshapedEncoderHiddenState = trainedImgCaptionModel.get_layer('reshape')(encoderHiddenState)\n",
        "\n",
        "decoderPredAttention = trainedImgCaptionModel.get_layer('additive_attention')\n",
        "decoderPredContextVector, decoderPredAlignmentWeight = decoderPredAttention([decoderPredLSTMOutput, reshapedEncoderHiddenState, reshapedEncoderHiddenState], return_attention_scores=True)\n",
        "\n",
        "decoderPredOutput = trainedImgCaptionModel.get_layer('dense_2')(decoderPredContextVector)\n",
        "\n",
        "decoderPredictModel = Keras.Model(\n",
        "    [decoderPredInput] + [encoderOutputTensorAsState] + [encoderPredictModel.get_layer('conv2d_7').output],\n",
        "    [decoderPredOutput] + [decoderPredStates]\n",
        ")\n",
        "\n",
        "# saving decoder prediction model\n",
        "decoderPredictModel.save(\"imageCaptionDecoderModel.keras\")\n",
        "print(decoderPredictModel.summary())\n",
        "Keras.utils.plot_model(decoderPredictModel, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3bwg6dGLpb_"
      },
      "outputs": [],
      "source": [
        "with open('index_to_word.json', 'r') as fh:\n",
        "  jsonData = json.load(fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaHhrxe6EfJ1"
      },
      "outputs": [],
      "source": [
        "encoderOutput = encoderPredictModel.predict(imgTestArray)\n",
        "\n",
        "encoderPrediction = encoderOutput[0]\n",
        "encoderPredHiddenLayer = encoderOutput[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp0_NeePZHxx"
      },
      "outputs": [],
      "source": [
        "# Decoder Prediction initilizer\n",
        "wordIdx = np.ones((1,))\n",
        "stateH = encoderPrediction\n",
        "stateC = encoderPrediction\n",
        "\n",
        "print(\"WORD INDEX: \", wordIdx)\n",
        "print(\"stateH: \", stateH.shape)\n",
        "print(\"stateC: \", stateC.shape)\n",
        "hiddenStates = [stateH, stateC]\n",
        "for i in range(20):\n",
        "\n",
        "  decoderPrediction, hiddenStates = decoderPredictModel.predict([wordIdx] + [hiddenStates] + [encoderPredHiddenLayer])\n",
        "  wordIdx[0] = decoderPrediction.argmax()\n",
        "\n",
        "  print(wordIdx[0])\n",
        "\n",
        "  #wordIdx[0] = decoderPrediction[0].argmax()\n",
        "\n",
        "  '''\n",
        "  stateH = decoderPrediction[1][0]\n",
        "  stateC = decoderPrediction[1][1]\n",
        "  encoderPredStates = [stateH, stateC]\n",
        "  '''\n",
        "\n",
        "  #if wordIdx[0] == 2:\n",
        "  #  break\n",
        "  print(\"wordIdx[0]\", wordIdx[0] ,\" \",jsonData[str(int(wordIdx[0]))])\n",
        "\n",
        "#model.predict(imgTestArray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBPLPujbRM0p"
      },
      "outputs": [],
      "source": [
        "jsonData['4']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wuSgsg1Qu8a"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "qwert = Keras.Sequential()\n",
        "qwert.add(imgCaptionModel.get_layer(\"input_1\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"conv2d\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"conv2d_1\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"max_pooling2d\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"conv2d_2\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"conv2d_3\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"max_pooling2d_1\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"conv2d_4\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"conv2d_5\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"max_pooling2d_2\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"conv2d_6\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"conv2d_7\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"max_pooling2d_3\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"flatten\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"dropout\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"dense\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"dropout_1\"))\n",
        "qwert.add(imgCaptionModel.get_layer(\"dense_1\"))\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVBi7crGEB9m"
      },
      "outputs": [],
      "source": [
        "# Building the decoder model for prediction\n",
        "'''\n",
        "decoderPredInput = decoderInput\n",
        "decoderPredEmbedding = decoderEmbedding(decoderPredInput)\n",
        "\n",
        "decoderPredLSTM = decoderLSTM\n",
        "decoderPredLSTMOutput, decoderPredHidden, decoderPredCell = decoderPredLSTM(decoderPredEmbedding, initial_state=abc)\n",
        "\n",
        "decoderPredStates = [decoderPredHidden, decoderPredCell]\n",
        "\n",
        "decoderPredAttention = additiveAttention\n",
        "decoderPredContextVector, decoderPredAlignmentWeight = decoderPredAttention([decoderPredLSTMOutput, cNNHiddenState, cNNHiddenState], return_attention_scores=True)\n",
        "\n",
        "decoderPredOutput = decoderOutputDenseLayer(decoderPredContextVector)\n",
        "\n",
        "decoderPredictModel = Keras.Model(\n",
        "    [decoderPredInput] + [abc] + [cNNHiddenState],\n",
        "    [decoderPredOutput] + [decoderPredStates]\n",
        ")\n",
        "\n",
        "# saving decoder prediction model\n",
        "decoderPredictModel.save(\"imageCaptionDecoderModel.keras\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fannHrhfJizc"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Building the encoder model for prediction\n",
        "encoderPredictModel = Keras.Model(\n",
        "    inputs=encoderInput,\n",
        "    outputs=[encoderOutput, cNNHiddenState]\n",
        ")\n",
        "abc = [encoderOutput, encoderOutput]\n",
        "\n",
        "encoderPredictModel.save(\"imageCaptionEncoderModel.keras\")\n",
        "encoderPredictModel.summary()\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}